# Recovery Plan: 241ìœ„ â†’ ìƒìœ„ 20% ëŒíŒŒ

> **ì‘ì„±ì¼:** 2025-12-15
> **í˜„ì¬:** 241/1006ìœ„ (Public 16.36)
> **ëª©í‘œ:** ìƒìœ„ 20% (Public < 16.0)
> **ì „ëµ:** Gradient Boosting
> **í™•ë¥ :** 60-70%

---

## ğŸ¯ ëª©í‘œ

### ë‹¨ê³„ë³„ ëª©í‘œ

| ë‹¨ê³„ | ëª©í‘œ | Public | ìˆœìœ„ | í™•ë¥  |
|------|------|--------|------|------|
| **Phase 1** | GBM Baseline | 15.5-16.0 | ~220ìœ„ | 80% |
| **Phase 2** | Feature + Tune | 15.0-15.5 | ~180ìœ„ | 60% |
| **Phase 3** | Ensemble | < 15.0 | ~150ìœ„ | 40% |

### ìµœì¢… ëª©í‘œ

```
Public < 16.0 (ìƒìœ„ 20%)
ê°œì„ : -0.36ì  (2.2%)
ê¸°í•œ: 2026-01-12 (D-28)
```

---

## ğŸ“… íƒ€ì„ë¼ì¸

### Week 2 ë‚¨ì€ ê¸°ê°„ (D-28~D-22) - í˜„ì¬

**ëª©í‘œ:** ì¤€ë¹„ ì™„ë£Œ

**í•  ì¼:**

1. âœ… Ultrathink ë¶„ì„ (ì™„ë£Œ)
2. âœ… Recovery Plan ì‘ì„± (ì§„í–‰ ì¤‘)
3. ğŸ”„ ë¹ ë¥¸ ì‹¤í—˜ ì‹œìŠ¤í…œ êµ¬ì¶•
4. ğŸ”„ GBM Baseline ì½”ë“œ ì‘ì„± (10% ìƒ˜í”Œ)
5. â¸ï¸ ê´€ì°° ëª¨ë“œ ìœ ì§€ (ì œì¶œ 0íšŒ)

**ì‚°ì¶œë¬¼:**
- `code/utils/fast_experiment.py` - 10% ìƒ˜í”Œë§, ìë™ CV
- `code/models/active/gbm_baseline.py` - XGBoost baseline
- `docs/EXPERIMENT_LOG.md` - ì‹¤í—˜ ê¸°ë¡ í…œí”Œë¦¿

### Week 3 (D-21~D-15)

**ëª©í‘œ:** Phase 1 ì™„ë£Œ (GBM Baseline)

**Day 1-2 (D-21~D-20):**
- XGBoost, LightGBM, CatBoost ë¹„êµ (10% ìƒ˜í”Œ)
- ê¸°ë³¸ í”¼ì²˜ (Zone 6x6 ìˆ˜ì¤€)
- ëª©í‘œ CV: 15.5-16.5

**Day 3-4 (D-19~D-18):**
- ìµœê³  ëª¨ë¸ ì„ íƒ
- Full data í•™ìŠµ
- CV ê²€ì¦ (5-fold)

**Day 5-6 (D-17~D-16):**
- ì²« ì œì¶œ (ê²€ì¦ ëª©ì )
- Gap í™•ì¸
- ì „ëµ ì¡°ì •

**Day 7 (D-15):**
- íœ´ì‹ & ì •ë¦¬
- Phase 2 ì¤€ë¹„

**ì œì¶œ:** 1-2íšŒ (ê²€ì¦ ëª©ì ë§Œ)

**ì‚°ì¶œë¬¼:**
- `code/models/active/gbm_v1.py` - Best GBM baseline
- Submission (ê²€ì¦ìš©)
- ì‹¤í—˜ ë¡œê·¸

### Week 4 (D-14~D-8)

**ëª©í‘œ:** Phase 2 ì™„ë£Œ (Feature + Tune)

**Day 1-3 (D-14~D-12):**
- Feature engineering ê°•í™”
  - ì‹œê°„ í”¼ì²˜ (period, time_left, pressure)
  - Episode í”¼ì²˜ (position, early/late)
  - Interaction í”¼ì²˜ (zone_time, zone_position)
- ëª©í‘œ CV: 14.5-15.5

**Day 4-5 (D-11~D-10):**
- Hyperparameter tuning
  - Grid search ë˜ëŠ” Optuna
  - CV ìµœì í™”
- ëª©í‘œ CV: 14.0-15.0

**Day 6-7 (D-9~D-8):**
- Full data í•™ìŠµ
- ì œì¶œ & ê²€ì¦
- Gap í™•ì¸

**ì œì¶œ:** 2-3íšŒ

**ì‚°ì¶œë¬¼:**
- `code/models/active/gbm_v2_features.py`
- `code/models/active/gbm_v3_tuned.py`
- ì‹¤í—˜ ë¹„êµ í‘œ

### Week 5 (D-7~D-0)

**ëª©í‘œ:** Phase 3 ì™„ë£Œ (Ensemble) & ìµœì¢… ì œì¶œ

**Day 1-2 (D-7~D-6):**
- Zone 10x10 ì‹¤í—˜
- Quantile regression
- ëª©í‘œ CV: 14.5-15.5

**Day 3-4 (D-5~D-4):**
- Ensemble êµ¬ì„±
  - Zone 6x6 (16.36) ê°€ì¤‘ì¹˜ 0.2
  - GBM best (15.0) ê°€ì¤‘ì¹˜ 0.6
  - Zone 10x10 (15.5) ê°€ì¤‘ì¹˜ 0.2
- ëª©í‘œ CV: 14.0-14.5

**Day 5-6 (D-3~D-2):**
- ìµœì¢… ê²€ì¦
- ì—¬ëŸ¬ ì¡°í•© ì‹œë„
- ìµœê³  ì„±ëŠ¥ ì„ íƒ

**Day 7 (D-1~D-0):**
- ìµœì¢… ì œì¶œ
- ë°±ì—… ì œì¶œ

**ì œì¶œ:** 5-10íšŒ (ì§‘ì¤‘)

**ì‚°ì¶œë¬¼:**
- `code/models/best/gbm_ensemble.py`
- Final submissions
- ìµœì¢… ë³´ê³ ì„œ

---

## ğŸ› ï¸ Phase 1: GBM Baseline (ìƒì„¸)

### ëª©í‘œ

```
CV: 15.5-16.5
Public: 15.5-16.5 (Gap < 1.0)
ê°œì„ : -0.5~-1.0ì 
ìˆœìœ„: ~220ìœ„
```

### 1.1 ë¹ ë¥¸ ì‹¤í—˜ ì‹œìŠ¤í…œ êµ¬ì¶•

**íŒŒì¼:** `code/utils/fast_experiment.py`

```python
"""
ë¹ ë¥¸ ì‹¤í—˜ ì‹œìŠ¤í…œ

Carla ì¡°ì–¸:
- 10% ìƒ˜í”Œë¡œ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
- ë©”ëª¨ë¦¬ ì£¼ì˜
- ìì£¼ ì €ì¥
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import GroupKFold
import time
import json

class FastExperiment:
    """ë¹ ë¥¸ ì‹¤í—˜ì„ ìœ„í•œ ìœ í‹¸ë¦¬í‹°"""

    def __init__(self, sample_frac=0.1, n_folds=3, random_state=42):
        self.sample_frac = sample_frac
        self.n_folds = n_folds
        self.random_state = random_state

    def load_data(self, sample=True):
        """ë°ì´í„° ë¡œë“œ (ìƒ˜í”Œë§ ì˜µì…˜)"""
        train_df = pd.read_csv('train.csv')

        if sample:
            # Episode ë‹¨ìœ„ ìƒ˜í”Œë§ (ì¤‘ìš”!)
            episodes = train_df['game_episode'].unique()
            sampled_episodes = np.random.choice(
                episodes,
                size=int(len(episodes) * self.sample_frac),
                replace=False
            )
            train_df = train_df[train_df['game_episode'].isin(sampled_episodes)]
            print(f"  Sampled: {len(sampled_episodes)} episodes ({self.sample_frac*100:.0f}%)")

        return train_df

    def create_features(self, df):
        """í”¼ì²˜ ìƒì„± (Episode ë…ë¦½ì„± ìœ ì§€)"""
        df = df.copy()

        # Zone 6x6
        df['zone_x'] = (df['start_x'] / (105/6)).astype(int).clip(0, 5)
        df['zone_y'] = (df['start_y'] / (68/6)).astype(int).clip(0, 5)
        df['zone'] = df['zone_x'].astype(str) + '_' + df['zone_y'].astype(str)

        # Direction 8-way
        df['dx'] = df['end_x'] - df['start_x']
        df['dy'] = df['end_y'] - df['start_y']
        df['prev_dx'] = df.groupby('game_episode')['dx'].shift(1).fillna(0)
        df['prev_dy'] = df.groupby('game_episode')['dy'].shift(1).fillna(0)

        angle = np.degrees(np.arctan2(df['prev_dy'], df['prev_dx']))
        df['direction'] = ((angle + 22.5) // 45).astype(int) % 8

        # Goal
        df['goal_distance'] = np.sqrt((105 - df['start_x'])**2 + (34 - df['start_y'])**2)
        df['goal_angle'] = np.degrees(np.arctan2(34 - df['start_y'], 105 - df['start_x']))

        # Time
        df['time_left'] = 5400 - df['time_seconds']

        # Episode
        df['pass_count'] = df.groupby('game_episode').cumcount() + 1

        return df

    def run_cv(self, model, X, y, groups, feature_names=None):
        """Cross-validation"""
        gkf = GroupKFold(n_splits=self.n_folds)
        fold_scores = []

        for fold, (train_idx, val_idx) in enumerate(gkf.split(X, y, groups=groups)):
            X_train, X_val = X[train_idx], X[val_idx]
            y_train, y_val = y[train_idx], y[val_idx]

            model.fit(X_train, y_train)
            pred = model.predict(X_val)

            # Euclidean distance
            dist = np.sqrt((pred[:, 0] - y_val[:, 0])**2 +
                          (pred[:, 1] - y_val[:, 1])**2)
            cv = dist.mean()
            fold_scores.append(cv)

            print(f"  Fold {fold+1}: {cv:.4f}")

        mean_cv = np.mean(fold_scores)
        std_cv = np.std(fold_scores)
        print(f"\n  Mean CV: {mean_cv:.4f} Â± {std_cv:.4f}")

        return mean_cv, std_cv, fold_scores

    def log_experiment(self, name, cv, params, features, runtime):
        """ì‹¤í—˜ ë¡œê·¸ ì €ì¥"""
        log = {
            'name': name,
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'cv_mean': cv[0],
            'cv_std': cv[1],
            'cv_folds': cv[2],
            'params': params,
            'features': features,
            'runtime': runtime,
            'sample_frac': self.sample_frac
        }

        # Append to log file
        with open('experiment_log.json', 'a') as f:
            f.write(json.dumps(log) + '\n')

        return log

    def compare_experiments(self, log_file='experiment_log.json'):
        """ì‹¤í—˜ ë¹„êµ í…Œì´ë¸”"""
        logs = []
        with open(log_file, 'r') as f:
            for line in f:
                logs.append(json.loads(line))

        # Sort by CV
        logs = sorted(logs, key=lambda x: x['cv_mean'])

        print("\n" + "=" * 80)
        print("ì‹¤í—˜ ë¹„êµ")
        print("=" * 80)
        print(f"{'Rank':<5} {'Name':<20} {'CV':<10} {'Runtime':<10} {'Sample':<10}")
        print("-" * 80)

        for i, log in enumerate(logs):
            print(f"{i+1:<5} {log['name']:<20} {log['cv_mean']:<10.4f} "
                  f"{log['runtime']:<10.1f}s {log['sample_frac']*100:<10.0f}%")

        return logs
```

### 1.2 GBM Baseline êµ¬í˜„

**íŒŒì¼:** `code/models/active/gbm_baseline.py`

```python
"""
GBM Baseline

3ê°œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¹„êµ:
- XGBoost
- LightGBM
- CatBoost
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import GroupKFold
import xgboost as xgb
import lightgbm as lgb
import catboost as cb
import time

import sys
sys.path.append('../../utils')
from fast_experiment import FastExperiment

print("=" * 80)
print("GBM Baseline Comparison")
print("=" * 80)

# Setup
exp = FastExperiment(sample_frac=0.1, n_folds=3)

# Load data
print("\n[1] ë°ì´í„° ë¡œë“œ...")
train_df = exp.load_data(sample=True)

# Features
print("\n[2] í”¼ì²˜ ìƒì„±...")
train_df = exp.create_features(train_df)

# Last pass per episode
train_last = train_df.groupby('game_episode').last().reset_index()

# Feature columns
feature_cols = [
    'start_x', 'start_y',
    'zone_x', 'zone_y',
    'direction',
    'goal_distance', 'goal_angle',
    'period_id', 'time_seconds', 'time_left',
    'pass_count',
    'prev_dx', 'prev_dy'
]

X = train_last[feature_cols].values
y = train_last[['end_x', 'end_y']].values
groups = train_last['game_episode'].str.split('_').str[0].values

print(f"  X: {X.shape}")
print(f"  y: {y.shape}")
print(f"  Features: {len(feature_cols)}")

# =============================================================================
# XGBoost
# =============================================================================
print("\n" + "=" * 80)
print("[3] XGBoost")
print("=" * 80)

start = time.time()

# Separate models for x and y
xgb_x = xgb.XGBRegressor(
    n_estimators=100,
    max_depth=6,
    learning_rate=0.1,
    random_state=42,
    n_jobs=-1
)

xgb_y = xgb.XGBRegressor(
    n_estimators=100,
    max_depth=6,
    learning_rate=0.1,
    random_state=42,
    n_jobs=-1
)

# CV
gkf = GroupKFold(n_splits=3)
fold_scores = []

for fold, (train_idx, val_idx) in enumerate(gkf.split(X, y, groups=groups)):
    X_train, X_val = X[train_idx], X[val_idx]
    y_train, y_val = y[train_idx], y[val_idx]

    # Fit
    xgb_x.fit(X_train, y_train[:, 0])
    xgb_y.fit(X_train, y_train[:, 1])

    # Predict
    pred_x = xgb_x.predict(X_val)
    pred_y = xgb_y.predict(X_val)

    # Clip
    pred_x = np.clip(pred_x, 0, 105)
    pred_y = np.clip(pred_y, 0, 68)

    # Score
    dist = np.sqrt((pred_x - y_val[:, 0])**2 + (pred_y - y_val[:, 1])**2)
    cv = dist.mean()
    fold_scores.append(cv)

    print(f"  Fold {fold+1}: {cv:.4f}")

xgb_cv = np.mean(fold_scores)
xgb_std = np.std(fold_scores)
xgb_time = time.time() - start

print(f"\n  XGBoost CV: {xgb_cv:.4f} Â± {xgb_std:.4f}")
print(f"  Runtime: {xgb_time:.1f}s")

# Log
exp.log_experiment(
    name='xgb_baseline',
    cv=(xgb_cv, xgb_std, fold_scores),
    params={'n_estimators': 100, 'max_depth': 6, 'learning_rate': 0.1},
    features=feature_cols,
    runtime=xgb_time
)

# =============================================================================
# LightGBM
# =============================================================================
print("\n" + "=" * 80)
print("[4] LightGBM")
print("=" * 80)

start = time.time()

lgb_x = lgb.LGBMRegressor(
    n_estimators=100,
    max_depth=6,
    learning_rate=0.1,
    random_state=42,
    n_jobs=-1,
    verbose=-1
)

lgb_y = lgb.LGBMRegressor(
    n_estimators=100,
    max_depth=6,
    learning_rate=0.1,
    random_state=42,
    n_jobs=-1,
    verbose=-1
)

# CV
fold_scores = []

for fold, (train_idx, val_idx) in enumerate(gkf.split(X, y, groups=groups)):
    X_train, X_val = X[train_idx], X[val_idx]
    y_train, y_val = y[train_idx], y[val_idx]

    lgb_x.fit(X_train, y_train[:, 0])
    lgb_y.fit(X_train, y_train[:, 1])

    pred_x = np.clip(lgb_x.predict(X_val), 0, 105)
    pred_y = np.clip(lgb_y.predict(X_val), 0, 68)

    dist = np.sqrt((pred_x - y_val[:, 0])**2 + (pred_y - y_val[:, 1])**2)
    cv = dist.mean()
    fold_scores.append(cv)

    print(f"  Fold {fold+1}: {cv:.4f}")

lgb_cv = np.mean(fold_scores)
lgb_std = np.std(fold_scores)
lgb_time = time.time() - start

print(f"\n  LightGBM CV: {lgb_cv:.4f} Â± {lgb_std:.4f}")
print(f"  Runtime: {lgb_time:.1f}s")

exp.log_experiment(
    name='lgb_baseline',
    cv=(lgb_cv, lgb_std, fold_scores),
    params={'n_estimators': 100, 'max_depth': 6, 'learning_rate': 0.1},
    features=feature_cols,
    runtime=lgb_time
)

# =============================================================================
# CatBoost
# =============================================================================
print("\n" + "=" * 80)
print("[5] CatBoost")
print("=" * 80)

start = time.time()

cat_x = cb.CatBoostRegressor(
    iterations=100,
    depth=6,
    learning_rate=0.1,
    random_state=42,
    verbose=0
)

cat_y = cb.CatBoostRegressor(
    iterations=100,
    depth=6,
    learning_rate=0.1,
    random_state=42,
    verbose=0
)

# CV
fold_scores = []

for fold, (train_idx, val_idx) in enumerate(gkf.split(X, y, groups=groups)):
    X_train, X_val = X[train_idx], X[val_idx]
    y_train, y_val = y[train_idx], y[val_idx]

    cat_x.fit(X_train, y_train[:, 0])
    cat_y.fit(X_train, y_train[:, 1])

    pred_x = np.clip(cat_x.predict(X_val), 0, 105)
    pred_y = np.clip(cat_y.predict(X_val), 0, 68)

    dist = np.sqrt((pred_x - y_val[:, 0])**2 + (pred_y - y_val[:, 1])**2)
    cv = dist.mean()
    fold_scores.append(cv)

    print(f"  Fold {fold+1}: {cv:.4f}")

cat_cv = np.mean(fold_scores)
cat_std = np.std(fold_scores)
cat_time = time.time() - start

print(f"\n  CatBoost CV: {cat_cv:.4f} Â± {cat_std:.4f}")
print(f"  Runtime: {cat_time:.1f}s")

exp.log_experiment(
    name='cat_baseline',
    cv=(cat_cv, cat_std, fold_scores),
    params={'iterations': 100, 'depth': 6, 'learning_rate': 0.1},
    features=feature_cols,
    runtime=cat_time
)

# =============================================================================
# Comparison
# =============================================================================
print("\n" + "=" * 80)
print("ìµœì¢… ë¹„êµ")
print("=" * 80)

results = [
    ('XGBoost', xgb_cv, xgb_std, xgb_time),
    ('LightGBM', lgb_cv, lgb_std, lgb_time),
    ('CatBoost', cat_cv, cat_std, cat_time)
]

results = sorted(results, key=lambda x: x[1])

print(f"{'Rank':<5} {'Model':<12} {'CV':<12} {'Runtime':<10}")
print("-" * 50)
for i, (name, cv, std, runtime) in enumerate(results):
    print(f"{i+1:<5} {name:<12} {cv:.4f}Â±{std:.4f}  {runtime:.1f}s")

best = results[0]
print(f"\nâœ… Best: {best[0]} (CV {best[1]:.4f})")

print("\në‹¤ìŒ ë‹¨ê³„:")
print("1. Full dataë¡œ í•™ìŠµ")
print("2. ì œì¶œ & Gap í™•ì¸")
print("3. Feature engineering")
```

### 1.3 ì‹¤í–‰ ìˆœì„œ

```bash
# 1. ë¹ ë¥¸ ì‹¤í—˜ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
cd code/utils
python fast_experiment.py

# 2. GBM Baseline (10% ìƒ˜í”Œ)
cd ../models/active
python gbm_baseline.py

# ì˜ˆìƒ ê²°ê³¼:
# XGBoost: CV ~15.5-16.5
# LightGBM: CV ~15.5-16.5
# CatBoost: CV ~15.5-16.5
# Runtime: 10-30s (10% ìƒ˜í”Œ)

# 3. Best ëª¨ë¸ë¡œ Full data
# gbm_baseline.pyì—ì„œ sample_frac=1.0ìœ¼ë¡œ ë³€ê²½
# Runtime: 2-5ë¶„ (ì „ì²´ ë°ì´í„°)
```

---

## ğŸ“Š Phase 2: Feature Engineering (Week 4)

### ì¶”ê°€ í”¼ì²˜ (Episode ë…ë¦½ì„± ìœ ì§€!)

```python
# 1. ì‹œê°„ í”¼ì²˜
df['is_first_half'] = (df['period_id'] == 1).astype(int)
df['is_last_10min'] = (df['time_left'] < 600).astype(int)
df['time_pressure'] = np.clip(600 - df['time_left'], 0, 600) / 600

# 2. ìœ„ì¹˜ í”¼ì²˜
df['is_attacking_third'] = (df['start_x'] > 70).astype(int)
df['is_defensive_third'] = (df['start_x'] < 35).astype(int)
df['is_central'] = ((df['start_y'] > 23) & (df['start_y'] < 45)).astype(int)
df['distance_from_sideline'] = np.minimum(df['start_y'], 68 - df['start_y'])

# 3. Episode í”¼ì²˜
df['episode_length'] = df.groupby('game_episode')['game_episode'].transform('size')
df['episode_position'] = df['pass_count'] / df['episode_length']
df['is_early_pass'] = (df['episode_position'] < 0.2).astype(int)
df['is_late_pass'] = (df['episode_position'] > 0.8).astype(int)

# 4. ì´ì „ íŒ¨ìŠ¤ í”¼ì²˜
df['prev_distance'] = np.sqrt(df['prev_dx']**2 + df['prev_dy']**2)
df['prev_angle'] = np.degrees(np.arctan2(df['prev_dy'], df['prev_dx']))

df['cumulative_dx'] = df.groupby('game_episode')['dx'].cumsum()
df['cumulative_dy'] = df.groupby('game_episode')['dy'].cumsum()

# 5. Interaction í”¼ì²˜
df['zone_time'] = df['zone'].astype(str) + '_' + df['period_id'].astype(str)
df['zone_position_bin'] = df['zone'].astype(str) + '_' + (df['episode_position'] * 5).astype(int).astype(str)
df['goal_dist_time'] = df['goal_distance'] * (1 + df['time_pressure'])
```

**ì¤‘ìš”:** ëª¨ë“  í”¼ì²˜ê°€ `groupby('game_episode')` ë‚´ë¶€ì—ì„œ ê³„ì‚°ë¨!

---

## ğŸ¯ Phase 3: Ensemble (Week 5)

### Ensemble ì „ëµ

```python
# 1. Zone 6x6 (ì•ˆì •ì , Gap +0.02)
zone_pred = zone_6x6_model.predict(test)

# 2. GBM Best (ì„±ëŠ¥, Gap ~1.0 ì˜ˆìƒ)
gbm_pred = gbm_best_model.predict(test)

# 3. Zone 10x10 (ì ˆì¶©, Gap ~0.5 ì˜ˆìƒ)
zone10_pred = zone_10x10_model.predict(test)

# Weighted ensemble
final_pred = (
    0.2 * zone_pred +
    0.6 * gbm_pred +
    0.2 * zone10_pred
)
```

### ê°€ì¤‘ì¹˜ ìµœì í™”

```python
from scipy.optimize import minimize

def objective(weights):
    """CV ìµœì†Œí™”"""
    pred = (weights[0] * zone_pred +
            weights[1] * gbm_pred +
            weights[2] * zone10_pred)
    cv = euclidean_distance(pred, y_true).mean()
    return cv

# Constraints: sum(weights) = 1, all >= 0
result = minimize(
    objective,
    x0=[0.33, 0.33, 0.33],
    constraints={'type': 'eq', 'fun': lambda w: w.sum() - 1},
    bounds=[(0, 1)] * 3
)

optimal_weights = result.x
```

---

## âœ… ì²´í¬ë¦¬ìŠ¤íŠ¸

### Phase 1 (GBM Baseline)

- [ ] `fast_experiment.py` ì‘ì„± ë° í…ŒìŠ¤íŠ¸
- [ ] `gbm_baseline.py` ì‘ì„±
- [ ] 10% ìƒ˜í”Œ ì‹¤í—˜ (3ê°œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¹„êµ)
- [ ] Best ëª¨ë¸ ì„ íƒ
- [ ] Full data í•™ìŠµ
- [ ] CV ê²€ì¦ (5-fold)
- [ ] ì²« ì œì¶œ & Gap í™•ì¸
- [ ] ì‹¤í—˜ ë¡œê·¸ ì •ë¦¬

### Phase 2 (Feature + Tune)

- [ ] ì‹œê°„ í”¼ì²˜ ì¶”ê°€
- [ ] ìœ„ì¹˜ í”¼ì²˜ ì¶”ê°€
- [ ] Episode í”¼ì²˜ ì¶”ê°€
- [ ] Interaction í”¼ì²˜ ì¶”ê°€
- [ ] CV ê°œì„  í™•ì¸
- [ ] Hyperparameter tuning
- [ ] ì œì¶œ & Gap í™•ì¸

### Phase 3 (Ensemble)

- [ ] Zone 10x10 êµ¬í˜„
- [ ] Quantile regression ì‹¤í—˜
- [ ] Ensemble êµ¬ì„±
- [ ] ê°€ì¤‘ì¹˜ ìµœì í™”
- [ ] ìµœì¢… ê²€ì¦
- [ ] ìµœì¢… ì œì¶œ

---

## ğŸ“ ì œì¶œ ì „ ì²´í¬ë¦¬ìŠ¤íŠ¸ (í•„ìˆ˜!)

### Episode ë…ë¦½ì„± í™•ì¸

- [ ] ëª¨ë“  í”¼ì²˜ê°€ `groupby('game_episode')` ì‚¬ìš©?
- [ ] Train/Test ë™ì¼ ë°©ì‹ ì²˜ë¦¬?
- [ ] ë‹¤ë¥¸ episode ì •ë³´ ì‚¬ìš© ì•ˆ í•¨?
- [ ] Cross-validation GroupKFold ì‚¬ìš©?

### ëŒ€íšŒ ê·œì¹™ í™•ì¸

- [ ] ì™¸ë¶€ ë°ì´í„° ì‚¬ìš© ì•ˆ í•¨?
- [ ] API í˜¸ì¶œ ì•ˆ í•¨?
- [ ] 2025.11.23 ì´ì „ ëª¨ë¸ë§Œ ì‚¬ìš©?
- [ ] ì½”ë“œ + ê°€ì¤‘ì¹˜ ì œì¶œ ê°€ëŠ¥?

### ì œì¶œ íŒŒì¼ ê²€ì¦

- [ ] ìƒ˜í”Œ ìˆ˜: 2,414ê°œ?
- [ ] ì»¬ëŸ¼: game_episode, end_x, end_y?
- [ ] NaN ì—†ìŒ?
- [ ] ë²”ìœ„: end_x [0, 105], end_y [0, 68]?
- [ ] ì¤‘ë³µ game_episode ì—†ìŒ?

---

## ğŸš¨ ìœ„í—˜ ê´€ë¦¬

### Risk 1: GBM Gap í´ ê²½ìš°

**ì¦ìƒ:** CV 15.0 â†’ Public 17.0 (Gap +2.0)

**ëŒ€ì‘:**
1. Feature ë‹¨ìˆœí™”
2. Regularization ê°•í™” (max_depth ê°ì†Œ, min_child_weight ì¦ê°€)
3. Ensemble ë¹„ì¤‘ ì¡°ì • (Zone 6x6 ë¹„ì¤‘ ì¦ê°€)

### Risk 2: CV ê°œì„  ì•ˆ ë  ê²½ìš°

**ì¦ìƒ:** GBM CV ~16.0 (Zone 6x6 ìˆ˜ì¤€)

**ëŒ€ì‘:**
1. Feature engineering ì¬ê²€í† 
2. ë‹¤ë¥¸ ëª¨ë¸ ì‹œë„ (Neural Network, TabNet)
3. Zone ì„¸ë¶„í™” (10x10, 12x12)

### Risk 3: ì‹œê°„ ë¶€ì¡±

**ì¦ìƒ:** D-3ì¸ë° ëª©í‘œ ë¯¸ë‹¬ì„±

**ëŒ€ì‘:**
1. Phase 3 ìƒëµ
2. Best single model ì œì¶œ
3. Zone 6x6 + Best GBM ê°„ë‹¨ ensemble

---

## ğŸ“ˆ ì„±ê³µ ì§€í‘œ

### Phase 1 ì„±ê³µ

```
10% ìƒ˜í”Œ: CV 15.5-16.5
Full data: CV 15.5-16.5
First submission: Public 15.5-16.5, Gap < 1.0

â†’ Phase 2 ì§„í–‰ âœ…
```

### Phase 2 ì„±ê³µ

```
Feature: CV 14.5-15.5 (-1.0 ê°œì„ )
Tune: CV 14.0-15.0 (-0.5 ê°œì„ )
Submission: Public 14.5-15.5, Gap < 1.0

â†’ Phase 3 ì§„í–‰ âœ…
```

### ìµœì¢… ì„±ê³µ

```
Ensemble: CV < 14.5
Final submission: Public < 16.0
ìˆœìœ„: ìƒìœ„ 20% (< 200ìœ„)

â†’ ëª©í‘œ ë‹¬ì„±! ğŸ‰
```

---

## ğŸ“ ì„±ê³µ ìš”ì¸

1. **Ultrathink ë¶„ì„:** ë¬¸ì œ ë³¸ì§ˆ ì´í•´ (í‘œì¤€í¸ì°¨ 15.9m)
2. **GBM ì„ íƒ:** Kaggle í‘œì¤€, tabular data ìµœê°•
3. **Episode ë…ë¦½ì„±:** Data Leakage ë°©ì§€
4. **ë¹ ë¥¸ ì‹¤í—˜:** 10% ìƒ˜í”Œë¡œ ë¹ ë¥¸ ë°˜ë³µ
5. **ì²´ê³„ì  ì ‘ê·¼:** Phaseë³„ ëª…í™•í•œ ëª©í‘œ

---

## ğŸ”— ì°¸ê³  ë¬¸ì„œ

- `docs/ULTRATHINK_ANALYSIS.md` - ë¬¸ì œ ë¶„ì„
- `docs/DATA_LEAKAGE_VERIFICATION.md` - ì•ˆì „ í™•ì¸
- `docs/AI_CODING_CONSTRAINTS.md` - ì œì•½ ì¡°ê±´
- `docs/COMPETITION_INFO.md` - ëŒ€íšŒ ê·œì •

---

**ì‘ì„±ì:** Claude Sonnet 4.5
**ì‘ì„±ì¼:** 2025-12-15
**ë‹¤ìŒ ë¦¬ë·°:** Phase 1 ì™„ë£Œ ì‹œ (Week 3 ë§)

---

*"The best way to predict the future is to create it."*
*"Zone 6x6ì€ ê³¼ê±°, GBMì€ ë¯¸ë˜. 241ìœ„ì—ì„œ ìƒìœ„ 20%ë¡œ!"*
