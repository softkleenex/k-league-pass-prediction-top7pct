# ëŒ€íšŒ ìš°ìŠ¹ì/ì°¸ê°€ì ì „ëµ ë¶„ì„

> **ì¶œì²˜:** Medium ê¸€ ë¶„ì„
> **ì‘ì„±ì¼:** 2025-12-15
> **ëª©ì :** ë‹¤ë¥¸ ëŒ€íšŒ ì°¸ê°€ìë“¤ì˜ ê²½í—˜ì—ì„œ ë°°ìš°ê¸°

---

## ğŸ“š ë¶„ì„í•œ ê¸€

### 1. My First Kaggle Competition - LLM Classification Finetuning

**ì €ì:** Carla Cotas
**ë§í¬:** https://medium.com/@carlacotas/my-first-kaggle-competition-llm-classification-finetuning-476db368b389
**ëŒ€íšŒ:** Kaggle LLM Classification Finetuning
**ì„±ê³¼:** ì²« ì°¸ê°€, 10ì£¼ ì±Œë¦°ì§€ ì™„ë£Œ

### 2. I Beat 400+ Data Scientists Using an AI That Kept Trying to Cheat â­ í•„ë…!

**ì €ì:** Nikhil Mishra (Kaggle Grandmaster, 40+ AI ëŒ€íšŒ ìš°ìŠ¹)
**ë§í¬:** https://medium.com/@devnikhilmishra/i-beat-400-to-win-lakhs-data-scientists-using-an-ai-that-kept-trying-to-cheat-fcb7add97d8a
**ëŒ€íšŒ:** RedBus í•´ì»¤í†¤
**ì„±ê³¼:** ìš°ìŠ¹ (400+ ì°¸ê°€ì, ìƒê¸ˆ 50ë§Œ ë£¨í”¼)
**ë„êµ¬:** Claude Code

---

## ğŸ¯ ëŒ€íšŒ ê°œìš”

### Kaggle LLM Classification Finetuning

**ëª©í‘œ:**
- Chatbot Arena ëŒ€í™”ì—ì„œ ì‚¬ìš©ì ì„ í˜¸ë„ ì˜ˆì¸¡
- ë‘ LLMì˜ ì‘ë‹µ ì¤‘ ì–´ëŠ ê²ƒì„ ì„ í˜¸í• ì§€ ì˜ˆì¸¡
- 3-class classification: model_a / model_b / tie

**ë°ì´í„°:**
- train.csv: id, model_a/b, prompt, response_a/b, winner
- test.csv: id, prompt, response_a/b
- í‰ê°€ ì§€í‘œ: **Log Loss**

**ì±Œë¦°ì§€:**
- Position bias (ì²« ë²ˆì§¸ ì‘ë‹µ ì„ í˜¸)
- Verbosity bias (ì¥í™©í•œ ì‘ë‹µ ì„ í˜¸)
- Self-enhancement bias (ìê¸° í™ë³´)

---

## ğŸ”¬ ì €ìì˜ ì ‘ê·¼ë²•

### 1ë‹¨ê³„: ë°ì´í„° ì´í•´ (Week 1)

```python
# ë°ì´í„° ë¡œë“œ
training = pd.read_csv('train.csv')
test = pd.read_csv('test.csv')

# ê¸°ë³¸ íƒìƒ‰
training.head(10)
training.tail(10)
```

**ì†Œìš” ì‹œê°„:** 1ì£¼
**ì–´ë ¤ì›€:** Kaggle í”Œë«í¼ ìµìˆ™í•´ì§€ê¸°
**í•´ê²°:** í† ë¡  í¬ëŸ¼ í™œìš©

### 2ë‹¨ê³„: ë°ì´í„° í´ë¦¬ë‹ (Week 2-3)

**í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬:**

```python
def clean_text(text):
    # ì†Œë¬¸ì ë³€í™˜
    text = text.lower()

    # ìˆ«ì ë° íŠ¹ìˆ˜ë¬¸ì ì œê±°
    text = re.sub(r'\d+', '', text)
    text = re.sub(r'[^\w\s]', '', text)

    # í† í°í™”
    text = nltk.word_tokenize(text)

    # ë¶ˆìš©ì–´ ì œê±°
    stop_words = set(stopwords.words('english'))
    text = [word for word in text if word not in stop_words]

    # í‘œì œì–´ ì¶”ì¶œ (Lemmatization)
    lemmatizer = nltk.WordNetLemmatizer()
    text = [lemmatizer.lemmatize(word) for word in text]

    return ' '.join(text)

# ì ìš©
training["prompt"] = training["prompt"].apply(clean_text)
training["response_a"] = training["response_a"].apply(clean_text)
training["response_b"] = training["response_b"].apply(clean_text)
```

**ì£¼ìš” ë°œê²¬:**
- ID ì¤‘ë³µ ì—†ìŒ âœ…
- NaN/Null ê°’ ì—†ìŒ âœ…
- 64ê°œ LLM ëª¨ë¸
- 5,743ê°œ ì¤‘ë³µ prompt (ì •ìƒ, ë‹¤ë¥¸ ëª¨ë¸ ì¡°í•©)

**ì‹¤ìˆ˜ ë° êµí›ˆ:**
- **ë…¸íŠ¸ë¶ í¬ë˜ì‹œ** â†’ ìì£¼ ì €ì¥í•˜ê¸°!
- Matplotlib ì„œë¸Œí”Œë¡¯ ì‹¤ìˆ˜ â†’ ê°ê° ë”°ë¡œ ê·¸ë¦¬ê¸°

### 3ë‹¨ê³„: ë°ì´í„° íƒìƒ‰ (Week 2-3)

**ì‹œê°í™”:**
- LLM ë¶„í¬ (model_a, model_b)
- Winner ë¶„í¬
- íŠ¹ë³„í•œ íŒ¨í„´ ë°œê²¬ ì•ˆ ë¨

**í•µì‹¬:**
- LLM ì •ë³´ëŠ” test ë°ì´í„°ì— ì—†ìŒ
- LLMë³„ ë¶„ì„ ì¤‘ë‹¨

### 4ë‹¨ê³„: í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ (Week 4)

**TF-IDF ë²¡í„°í™”:**

```python
from sklearn.feature_extraction.text import TfidfVectorizer

# ì¤‘ìš”: max_features ì„¤ì • (ë©”ëª¨ë¦¬ ì œí•œ!)
vectorizer = TfidfVectorizer(max_features=150)

# ê° í…ìŠ¤íŠ¸ í•„ë“œ ë²¡í„°í™”
vectorizer_prompt = vectorizer.fit_transform(training["prompt"])
vectorizer_response_a = vectorizer.fit_transform(training["response_a"])
vectorizer_response_b = vectorizer.fit_transform(training["response_b"])

# í”¼ì²˜ ê²°í•©
train_X = np.concatenate((
    temp_prompt.toarray(),
    temp_response_a.toarray(),
    temp_response_b.toarray()
), axis=1)

# íƒ€ê²Ÿ
train_y = training["winner"].values
```

**Critical Issue:**
- **ë©”ëª¨ë¦¬ ì œí•œ!** max_features ì—†ì´ ì‹¤í–‰ â†’ í¬ë˜ì‹œ
- **í•´ê²°:** max_features=150 ì„¤ì •

### 5ë‹¨ê³„: ëª¨ë¸ í•™ìŠµ (Week 5-6)

**Logistic Regression:**

```python
from sklearn.linear_model import LogisticRegression

model = LogisticRegression(
    max_iter=500,
    multi_class='multinomial',  # 3-class
    solver='saga'               # ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì— ë¹ ë¦„
)

model.fit(train_X, train_y)
```

**ì‹¤í–‰ ì‹œê°„ ì¸¡ì •:**

```python
from datetime import datetime

start = datetime.now()
# ... ëª¨ë¸ í•™ìŠµ ...
end = datetime.now()

execution_time = (end - start).total_seconds() / 60
print(f"Execution time: {execution_time} minutes")
```

### 6ë‹¨ê³„: ëª¨ë¸ í‰ê°€ (Week 5-6)

```python
# Train/Validation Split
train_X_train, train_X_val, train_y_train, train_y_val = train_test_split(
    train_X, train_y, test_size=0.2, random_state=42
)

# í‰ê°€
value_y_predict = model.predict(train_X_val)
value_y_probabilities = model.predict_proba(train_X_val)

# Confusion Matrix
cm = confusion_matrix(train_y_val, value_y_predict)

# Accuracy
score = model.score(train_X_val, train_y_val)  # < 50%

# Precision & Recall
macro_precision = precision_score(train_y_val, value_y_predict, average='macro')
macro_recall = recall_score(train_y_val, value_y_predict, average='macro')

# Log Loss (í•µì‹¬ ì§€í‘œ!)
model_log_loss = log_loss(train_y_val, value_y_probabilities)
# Result: 1.05 (baseline: log(1/3) = 1.10)
```

**í•µì‹¬ ì¸ì‚¬ì´íŠ¸:**
- Accuracy < 50% â†’ ì²˜ìŒì—” í˜¼ë€
- **Log Lossê°€ ë” ì¤‘ìš”!** 1.05 < 1.10 (good!)
- í† ë¡  í¬ëŸ¼ì—ì„œ Log Loss ì¤‘ìš”ì„± í•™ìŠµ

### 7ë‹¨ê³„: ì œì¶œ (Week 6)

```python
# ì˜ˆì¸¡
test_X = np.concatenate((
    temp_test_prompt.toarray(),
    temp_test_response_a.toarray(),
    temp_test_response_b.toarray()
), axis=1)

value_test_y_probabilities = model.predict_proba(test_X)

# ì œì¶œ íŒŒì¼
output = pd.DataFrame({
    'id': test.id,
    'winner_model_a': value_test_y_probabilities[:, 0],
    'winner_model_b': value_test_y_probabilities[:, 1],
    'winner_tie': value_test_y_probabilities[:, 2]
})

output.to_csv('submission.csv', index=False)
```

**ì²« ì œì¶œ ì ìˆ˜:** 1.11623 (í•˜ìœ„ê¶Œ)

---

## ğŸ’¡ í•µì‹¬ êµí›ˆ

### 1. í”Œë«í¼ ìµìˆ™í•´ì§€ê¸°

```
âœ… Kaggle/DACON í† ë¡  í¬ëŸ¼ ì ê·¹ í™œìš©
âœ… ë…¸íŠ¸ë¶ ìì£¼ ì €ì¥ (í¬ë˜ì‹œ ëŒ€ë¹„)
âœ… ìƒ˜í”Œ ì œì¶œ íŒŒì¼ í™•ì¸
```

### 2. í…ìŠ¤íŠ¸ ë°ì´í„° ì²˜ë¦¬

```
âœ… ì „ì²˜ë¦¬ í•„ìˆ˜ (ì†Œë¬¸ì, íŠ¹ìˆ˜ë¬¸ì ì œê±°, ë¶ˆìš©ì–´, í‘œì œì–´)
âœ… TF-IDF ê°™ì€ ë²¡í„°í™” ê¸°ë²•
âœ… ë©”ëª¨ë¦¬ ì œí•œ ì£¼ì˜ (max_features ì„¤ì •)
```

### 3. ëª¨ë¸ ì„ íƒ

```
âœ… Multi-class â†’ Logistic Regression (multinomial)
âœ… ëŒ€ê·œëª¨ ë°ì´í„° â†’ solver='saga'
âœ… ì‹¤í–‰ ì‹œê°„ ì¸¡ì •ìœ¼ë¡œ ë³‘ëª© íŒŒì•…
```

### 4. í‰ê°€ ì§€í‘œ ì´í•´

```
âš ï¸ Accuracyê°€ ë‚®ì•„ë„ ê´œì°®ì„ ìˆ˜ ìˆìŒ
âœ… ëŒ€íšŒì˜ í•µì‹¬ ì§€í‘œ ì§‘ì¤‘ (Log Loss, Euclidean Distance ë“±)
âœ… í† ë¡  í¬ëŸ¼ì—ì„œ ì§€í‘œ í•´ì„ í•™ìŠµ
```

### 5. ë°˜ë³µì  ê°œì„ 

```
âœ… ì²« ì œì¶œì´ í•˜ìœ„ê¶Œì´ì–´ë„ í•™ìŠµ ê³¼ì •ì´ ì¤‘ìš”
âœ… í† ë¡  í¬ëŸ¼ì—ì„œ ë‹¤ë¥¸ ì°¸ê°€ì ì „ëµ í•™ìŠµ
âœ… ì‘ì€ ë³€ê²½ì˜ ì˜í–¥ í…ŒìŠ¤íŠ¸
```

---

## ğŸ¯ ìš°ë¦¬ Kë¦¬ê·¸ ëŒ€íšŒì— ì ìš©

### ì§ì ‘ ì ìš© ê°€ëŠ¥

1. **í† ë¡ /ì½”ë“œ ê³µìœ  í™œìš©**
   ```
   í˜„ì¬: ê±°ì˜ í™œìš© ì•ˆ í•¨
   ê°œì„ : DACON í† ë¡  ê²Œì‹œíŒ, ì½”ë“œ ê³µìœ  ì ê·¹ í™•ì¸
   ```

2. **ì‹¤í–‰ ì‹œê°„ ì¸¡ì •**
   ```python
   # ë³‘ëª© êµ¬ê°„ íŒŒì•…
   start = datetime.now()
   # ... í•™ìŠµ ...
   end = datetime.now()
   print(f"Time: {(end-start).total_seconds()} sec")
   ```

3. **ë©”ëª¨ë¦¬ ìµœì í™”**
   ```
   ìœ ì‚¬ ì‚¬ë¡€: LSTM v5 íŒŒë¼ë¯¸í„° 74.6% ê°ì†Œ
   êµí›ˆ: ë©”ëª¨ë¦¬ë§Œì´ ì•„ë‹ˆë¼ ì„±ëŠ¥ë„ ê³ ë ¤
   ```

4. **ì²´ê³„ì  ë‹¨ê³„ë³„ ì ‘ê·¼**
   ```
   Week 1: ë°ì´í„° ì´í•´ âœ…
   Week 2-3: í´ë¦¬ë‹ & íƒìƒ‰ âœ…
   Week 4: í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ âœ…
   Week 5-6: ëª¨ë¸ë§ âœ…

   ìš°ë¦¬: ì´ë¯¸ ì˜í•˜ê³  ìˆìŒ! âœ…
   ```

### ì°¨ì´ì  & ë°°ìš¸ ì 

| í•­ëª© | Carlaì˜ ëŒ€íšŒ | ìš°ë¦¬ ëŒ€íšŒ | ì ìš© |
|------|--------------|-----------|------|
| **ë°ì´í„°** | í…ìŠ¤íŠ¸ (LLM ì‘ë‹µ) | ìˆ˜ì¹˜ (ì¢Œí‘œ) | - |
| **í‰ê°€** | Log Loss | Euclidean Distance | ì§€í‘œ ì´í•´ ì¤‘ìš” |
| **ì ‘ê·¼** | TF-IDF + Logistic Regression | Zone í†µê³„, LSTM | ë‹¨ìˆœí•¨ ìš°ì„  |
| **ê¸°ê°„** | 10ì£¼ ì±Œë¦°ì§€ | 6ì£¼ (43ì¼) | ì§‘ì¤‘ í•„ìš” |
| **ì²« ì œì¶œ** | í•˜ìœ„ê¶Œ (1.11623) | ì¤‘í•˜ìœ„ê¶Œ (16.36) | ê´œì°®ìŒ! |

### ìš°ë¦¬ê°€ ë” ì˜í•˜ëŠ” ì 

```
âœ… ì²´ê³„ì ì¸ ë¬¸ì„œí™” (28íšŒ ì‹¤í—˜ ê¸°ë¡)
âœ… CV/Public Gap ë¶„ì„ (Sweet Spot ë°œê²¬)
âœ… 14íšŒ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì™„ì „ íƒìƒ‰
âœ… ì‹¤íŒ¨ ë¶„ì„ ë¬¸ì„œí™” (LSTM 5ê°œ ë²„ì „)
```

### ìš°ë¦¬ê°€ ê°œì„ í•  ì 

```
âŒ í† ë¡  ê²Œì‹œíŒ í™œìš© ë¶€ì¡±
   â†’ DACON í† í¬, ì½”ë“œ ê³µìœ  í™•ì¸

âŒ ë‹¤ë¥¸ ì°¸ê°€ì ì ‘ê·¼ë²• ë¶„ì„ ë¶€ì¡±
   â†’ ìƒìœ„ê¶Œ ê³µê°œ ë…¸íŠ¸ë¶ í•™ìŠµ

âŒ ì‘ì€ ë³€ê²½ í…ŒìŠ¤íŠ¸ ë¶€ì¡±
   â†’ Week 4-5ì— ì‘ì€ ì‹¤í—˜ë“¤
```

---

## ğŸ“‹ ì‹¤í–‰ ê³„íš

### Week 2-3 (í˜„ì¬)

```
âœ… ë¬¸ì„œí™” ì™„ë£Œ
â–¡ DACON í† ë¡  ê²Œì‹œíŒ í™•ì¸ (ë§¤ì¼ 10ë¶„)
â–¡ ìƒìœ„ê¶Œ ì½”ë“œ ê³µìœ  1-2ê°œ ë¶„ì„
â–¡ ë‹¤ë¥¸ ì°¸ê°€ì ì ‘ê·¼ë²• ìš”ì•½
```

### Week 4-5 (í›„ë°˜ì „)

```
â–¡ í•™ìŠµí•œ ì „ëµ í…ŒìŠ¤íŠ¸ (ì‘ì€ ë³€ê²½)
â–¡ ì‹¤í–‰ ì‹œê°„ ì¸¡ì •ìœ¼ë¡œ ë³‘ëª© íŒŒì•…
â–¡ í† ë¡ ì—ì„œ ë°°ìš´ íŒ ì ìš©
â–¡ ì œì¶œ 2-4íšŒ/ì¼
```

---

## ğŸ”— ì°¸ê³  ìë£Œ

### Carlaì˜ ìë£Œ

- **Medium ê¸€:** [ë§í¬](https://medium.com/@carlacotas/my-first-kaggle-competition-llm-classification-finetuning-476db368b389)
- **Kaggle ë…¸íŠ¸ë¶:** ê³µê°œë¨
- **GitHub:** ê³µê°œë¨

### ìš°ë¦¬ ìë£Œ

- **ëŒ€íšŒ í† ë¡ :** https://dacon.io/competitions/official/236647/talkboard
- **ì½”ë“œ ê³µìœ :** https://dacon.io/competitions/official/236647/codeshare

---

## ğŸ“ í•µì‹¬ ë©”ì‹œì§€

```
"ì²« ì œì¶œì´ í•˜ìœ„ê¶Œì´ì–´ë„ ê´œì°®ë‹¤.
 10ì£¼ ì±Œë¦°ì§€ë¥¼ ì™„ë£Œí•˜ë©° ë°°ìš´ ê²ƒì´ ë” ì¤‘ìš”í•˜ë‹¤.

 ìš°ë¦¬ë„ ë§ˆì°¬ê°€ì§€ë‹¤.
 í˜„ì¬ 241ìœ„ì§€ë§Œ, 6ì£¼ê°„ ì²´ê³„ì ìœ¼ë¡œ ì ‘ê·¼í–ˆê³ 
 ë§ì€ ê²ƒì„ ë°°ì› ë‹¤.

 ì´ì œ í† ë¡ ì„ í™œìš©í•˜ê³ , ë‹¤ë¥¸ ì°¸ê°€ìì—ê²Œ ë°°ìš°ë©°
 í›„ë°˜ì „ì„ ì¤€ë¹„í•˜ì."
```

---

## ğŸ† Nikhilì˜ ìš°ìŠ¹ ì „ëµ (RedBus í•´ì»¤í†¤)

### ëŒ€íšŒ ê°œìš”

**ë¬¸ì œ:**
- 15ì¼ í›„ ë²„ìŠ¤ ì¢Œì„ ì˜ˆì•½ í˜„í™© ì˜ˆì¸¡
- í‰ê°€ ì§€í‘œ: RMSE
- ì œì•½: **15ì¼ ì „ ë°ì´í„°ë§Œ ì‚¬ìš© ê°€ëŠ¥**

**í•µì‹¬ ë„ì „:**
- ì‚¬ëŒë“¤ì€ ë²„ìŠ¤í‘œë¥¼ ë§‰íŒì— ì˜ˆì•½ (20% only ë©°ì¹  ì „)
- ìˆ˜í•™ìœ¼ë¡œ ì¸ê°„ì˜ ìë°œì„± ì˜ˆì¸¡í•´ì•¼ í•¨
- **ì‹œê°„ì  ì œì•½ ìœ„ë°˜ = Data Leakage = ì‹¤íŒ¨**

---

### ğŸš¨ Claude Codeì˜ 3ë²ˆ ì—°ì† Data Leakage ì‹¤íŒ¨!

#### 1-3ì°¨ ì œì¶œ: ì°¸íŒ¨

**ë¬¸ì œ:**
```python
# Claude Codeê°€ ë§Œë“  ì˜ëª»ëœ í”¼ì²˜
"í–¥í›„ 7ì¼ê°„ í‰ê·  ì˜ˆì•½ ê±´ìˆ˜"  # âŒ ë¯¸ë˜ ë°ì´í„° ì‚¬ìš©!
"ì£¼ì¤‘ ìµœëŒ€ ì˜ˆì•½ ìš”ì¼"          # âŒ ì—¬ì • ë‚ ì§œ ì´í›„ ì˜ˆì•½ê¹Œì§€ í¬í•¨!
```

**ì¦ìƒ:**
- Validation RMSE: í›Œë¥­ âœ…
- Public Leaderboard: í˜•í¸ì—†ìŒ âŒ

**ì›ì¸:**
- Claude Codeê°€ **ëª¨ë“  ë°ì´í„°**ë¥¼ ì‚¬ìš©í•´ì„œ í”¼ì²˜ ìƒì„±
- ì‹œê°„ì  ì œì•½ ì™„ì „íˆ ë¬´ì‹œ
- **ì „í˜•ì ì¸ Data Leakage**

#### ê¹¨ë‹¬ìŒ

> "Claude CodeëŠ” ëª¨ë“  ë°ì´í„°ë¥¼ í™œìš©í•˜ì—¬ í”¼ì²˜ë¥¼ ê°œë°œí–ˆìŠµë‹ˆë‹¤.
> ì—¬ê¸°ì—ëŠ” **ì—¬ì • ë‚ ì§œ ì´í›„ì— ë°œìƒí•œ ì˜ˆì•½ê¹Œì§€ í¬í•¨**ë˜ì—ˆìŠµë‹ˆë‹¤.
>
> ì „í˜•ì ì¸ ë°ì´í„° ìœ ì¶œ ì‚¬ë¡€. ê²½ìŸì‚¬ ìë©¸ í–‰ìœ„."

---

### âœ… í•´ê²°: ëª…ì‹œì  ì‹œê°„ ì œì•½

#### ëª¨ë“  ê²ƒì„ êµ¬í•œ í•œ ì¤„

```python
# The line that saved everything
trans_filt = transactions_df.filter((pl.col('dbd') >= 15))

# Then ALL feature engineering on this filtered data
features = trans_filt.group_by(['route', 'source', 'destination']).agg([
    pl.col('seats_booked').mean().alias('avg_seats'),
    pl.col('seats_booked').std().alias('std_seats'),
    # ... eventually 35,000+ features
])
```

**í•µì‹¬:**
```
âš ï¸ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ **ì „ì—** ì‹œê°„ ì œì•½ í•„í„°ë§!
âœ… 35,000+ í”¼ì²˜ ëª¨ë‘ 15ì¼ ì „ ë°ì´í„°ë§Œ ì‚¬ìš©
```

---

### ğŸ’¡ Claude Code = ë§¤ìš° ë˜‘ë˜‘í•œ Junior Engineer

#### Claude Codeê°€ ì˜í•˜ëŠ” ê²ƒ

```
âœ… ë³´ì¼ëŸ¬í”Œë ˆì´íŠ¸ ì½”ë“œ ì´ˆê³ ì† ì‘ì„±
âœ… ì˜ ì •ì˜ëœ êµ¬ì²´ì  ì‘ì—… êµ¬í˜„
âœ… ì‹¤í—˜ í”„ë ˆì„ì›Œí¬ êµ¬ì¶•
âœ… ìµœì‹  ë¼ì´ë¸ŒëŸ¬ë¦¬ ì§€ì‹ (TabDPT ê°™ì€ ê²ƒ)
```

#### Claude Codeê°€ ëª»í•˜ëŠ” ê²ƒ

```
âŒ ML ë¬¸ì œì˜ ì•”ë¬µì  ì œì•½ ì´í•´
âŒ ì‹œê°„ì  ê²€ì¦ ë° Data Leakage ë°©ì§€
âŒ ìµœì í™”ëœ ì½”ë“œ ì‘ì„± (pandas ê¸°ë³¸, Polars 10ë°° ë¹ ë¦„)
âŒ ë¹„ìŠ·í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ êµ¬ë¬¸ í˜¼ë™
```

---

### ğŸ“ CLAUDE.md: ê²Œì„ ì²´ì¸ì €

#### Nikhilì˜ CLAUDE.md

```markdown
# CRITICAL CONSTRAINTS:
- ALWAYS filter data with temporal constraints BEFORE feature creation
- Use only data from >= 15 days before prediction date
- No data leakage: future cannot predict past

# CODE PREFERENCES:
- Use Polars for large datasets, not pandas
- Iterate on smaller faster code
```

**íš¨ê³¼:**
> "Claude Codeê°€ ë„ë©”ì¸ ì „ë¬¸ ì§€ì‹ì„ ë¯¸ë¦¬ ë¡œë“œí•˜ì
> ì—„ì²­ë‚˜ê²Œ íš¨ê³¼ì ìœ¼ë¡œ ë³€í–ˆìŠµë‹ˆë‹¤."

---

### ğŸ”¬ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ í­ë°œ

#### 35,000+ í”¼ì²˜ ìƒì„±

**Nikhilì˜ ì§€ì‹œ:**
```
"ì‹œê°„ì  íŒ¨í„´ì„ í¬ì°©í•˜ëŠ” temporal í”¼ì²˜ë¥¼ ë§Œë“¤ì–´ì¤˜.
ì¤‘ìš”: ëª¨ë“  í”¼ì²˜ëŠ” ì‹œê°„ ì œì•½ì„ ì¤€ìˆ˜í•´ì•¼ í•¨.
df_filtë¥¼ ë² ì´ìŠ¤ë¡œ ì‚¬ìš©.
íœ´ì¼, ìš”ì¼ íš¨ê³¼, ê³„ì ˆ íŠ¸ë Œë“œë¥¼ ìƒê°í•´ë´."
```

**Claude Code ê²°ê³¼:**
```python
âœ… Cyclical encoding (sine/cosine ë³€í™˜)
âœ… íœ´ì¼ ê·¼ì ‘ë„ í”¼ì²˜ (ì¸ë„ íŠ¹ì • íœ´ì¼)
âœ… 10ê°œ ë‹¤ë¥¸ ì‹œê°„ êµ¬ê°„ì˜ ë¡¤ë§ ìœˆë„ìš° í†µê³„
âœ… ëª¨ë©˜í…€ ì ìˆ˜, íš¨ìœ¨ì„± ë¹„ìœ¨ ê°™ì€ 2ì°¨ í”¼ì²˜
```

**í•µì‹¬:**
```
ì œì•½ì€ ëª…ì‹œì ìœ¼ë¡œ
êµ¬í˜„ ì„¸ë¶€ì‚¬í•­ì€ Claudeì—ê²Œ
```

---

### ğŸ§ª ì‹¤í—˜ ì‹œìŠ¤í…œ

#### ë¹ ë¥¸ ë°˜ë³µ í”„ë ˆì„ì›Œí¬

```python
FEATURE_CONFIGS = [
    {'name': '1K_features', 'top_n_features': 1000},
    {'name': '2K_features', 'top_n_features': 2000},
    {'name': '3K_features', 'top_n_features': 3000},
    {'name': '6K_features', 'top_n_features': 6000}
]
```

**ì „ëµ:**
1. **10% ë°ì´í„°ë¡œ ë¹ ë¥¸ ì‹¤í—˜**
2. ìŠ¹ìë§Œ full-scale í•™ìŠµ
3. ê³„ì‚° ì‹œê°„ ì ˆì•½, 10ë°° ë¹ ë¥¸ ì•„ì´ë””ì–´ í…ŒìŠ¤íŠ¸

---

### ğŸ¯ TabDPT: ì„œí”„ë¼ì´ì¦ˆ ë¬´ê¸°

**Claude Code ì œì•ˆ:**
> "TabDPTëŠ” tabular ë°ì´í„°ì—ì„œ í›Œë¥­í•œ ê²°ê³¼ë¥¼ ë³´ì…ë‹ˆë‹¤.
> í†µí•©í•´ë“œë¦´ê¹Œìš”?"

**ê²°ê³¼:**
- RMSE 455.68
- ë‹¨ì¼ GBM ëª¨ë¸ë³´ë‹¤ í›¨ì”¬ ì¢‹ìŒ

**êµí›ˆ:**
```
âœ… AIëŠ” ë‹¹ì‹ ì´ (ì•„ì§) ëª¨ë¥´ëŠ” ê¸°ë²•ì„ ì•Œê³  ìˆìŒ
```

---

### ğŸ… ìµœì¢… ì•™ìƒë¸”

```python
# ê°€ì¤‘ ì•™ìƒë¸”
75% weight: Gradient Boosting ensemble (12 models)
25% weight: TabDPT predictions
```

**ì „ëµ:**
- ì „í†µì  ëª¨ë¸ì˜ ì•ˆì •ì„± í™œìš©
- Transformer í˜ì‹  ê²°í•©

---

### ğŸ’¼ Nikhilì˜ ì‹¤ì œ ì›Œí¬í”Œë¡œìš°

#### 1. CLAUDE.md ë¨¼ì € ì„¤ì •

```markdown
ì œì•½ ì¡°ê±´, ê²€ì¦ ì „ëµ, ì„ í˜¸ ë¼ì´ë¸ŒëŸ¬ë¦¬, í”í•œ ì‹¤ìˆ˜ ë¬¸ì„œí™”
```

#### 2. ë¹ ë¥¸ ì‹¤í—˜ ë£¨í”„ êµ¬ì¶•

```
10% ë°ì´í„° ìƒ˜í”Œë¡œ ë¹ ë¥¸ ë°˜ë³µ
â†’ ìŠ¹ìë§Œ full-scale í•™ìŠµìœ¼ë¡œ ìŠ¹ê²©
```

#### 3. êµ¬ì²´ì ì¸ ìš”ì²­

**ë‚˜ì¨:**
> "ìš°ìŠ¹ ì†”ë£¨ì…˜ì„ ë§Œë“¤ì–´ì¤˜"

**ì¢‹ìŒ:**
> "7, 14, 30ì¼ ë¡¤ë§ í†µê³„ë¥¼ ë§Œë“¤ì–´ì¤˜. ì‹œê°„ ì œì•½ ì¤€ìˆ˜í•´ì•¼ í•¨."

#### 4. 3ë‹¨ê³„ íŒŒì´í”„ë¼ì¸

```
íƒìƒ‰ (Exploration):
  Claudeê°€ ì´ˆê¸° í”¼ì²˜ ë¹ ë¥´ê²Œ ìƒì„±

ì‹¤í—˜ (Experimentation):
  ì‘ì€ ë°ì´í„°ë¡œ ì ‘ê·¼ë²• í…ŒìŠ¤íŠ¸

í”„ë¡œë•ì…˜ (Production):
  ê²€ì¦ëœ ì ‘ê·¼ë§Œ ìŠ¤ì¼€ì¼ì—…
```

---

## ğŸ¯ ìš°ë¦¬ Kë¦¬ê·¸ ëŒ€íšŒì— ì ìš© (ì¤‘ìš”!)

### ğŸš¨ Data Leakage ë°©ì§€ - ìµœìš°ì„ !

**ìš°ë¦¬ ëŒ€íšŒ ê·œì¹™:**
```
âŒ ëª¨ë“  ì˜ˆì¸¡ì€ game_id-episode ë‹¨ìœ„ë¡œ ë…ë¦½ì 
âŒ ë‹¤ë¥¸ ì—í”¼ì†Œë“œ ë°ì´í„° ì‚¬ìš© ê¸ˆì§€ (ë™ì¼ ê²½ê¸° ë‚´ ë‹¤ë¥¸ episode í¬í•¨)
```

**Nikhilì˜ êµí›ˆ ì ìš©:**
```python
# ìš°ë¦¬ë„ í•„ìš”í•œ í•„í„°!
# ê° episodeëŠ” ë…ë¦½ì ìœ¼ë¡œ ì˜ˆì¸¡
# ë‹¤ë¥¸ episode ì •ë³´ ì ˆëŒ€ ì‚¬ìš© ê¸ˆì§€

# ì˜ˆ: LSTM í•™ìŠµ ì‹œ
for episode_id in episodes:
    episode_data = data[data['episode'] == episode_id]  # âœ… ì´ episodeë§Œ
    # NOT: data[data['game_id'] == game_id]  # âŒ ê°™ì€ ê²½ê¸° ì „ì²´
```

**ìš°ë¦¬ê°€ ì´ë¯¸ ìœ„ë°˜í–ˆì„ ê°€ëŠ¥ì„±:**
```
âš ï¸ ì „ì²´ íŒ¨ìŠ¤ í•™ìŠµ (356K) - ë‹¤ë¥¸ episode ì •ë³´ ì‚¬ìš©?
âš ï¸ í™•ì¸ í•„ìš”: episode ë…ë¦½ì„± ë³´ì¥ë˜ëŠ”ì§€?
```

### CLAUDE.md ì‘ì„± (ì¦‰ì‹œ!)

#### ìš°ë¦¬ì˜ CLAUDE.md

```markdown
# Kë¦¬ê·¸ íŒ¨ìŠ¤ ì¢Œí‘œ ì˜ˆì¸¡ - CRITICAL CONSTRAINTS

## DATA LEAKAGE ë°©ì§€ (ìµœìš°ì„ !)
- ALWAYS predict each episode INDEPENDENTLY
- NEVER use data from other episodes (even same game_id)
- Filter by episode_id BEFORE any feature engineering
- No future information: only use passes BEFORE the target pass

## ëŒ€íšŒ ê·œì¹™
- No API calls (OpenAI, Gemini, etc.)
- No external data
- Only pretrained models from before 2025.11.23
- Local execution only

## ì½”ë“œ ì„ í˜¸
- Use Polars for large datasets
- Measure execution time for bottlenecks
- Document experiments in EXPERIMENT_LOG.md

## ê²€ì¦ ì „ëµ
- CV Sweet Spot: 16.27-16.34
- CV < 16.27 = Overfitting (Gap explosion)
- Target: Gap < 0.1

## ê¸ˆì§€ ì‚¬í•­
- NO LSTM (4ë²ˆ ì‹¤íŒ¨)
- NO data augmentation (Flip, Rotation)
- NO CV < 16.27 ì¶”êµ¬
```

### ë¹ ë¥¸ ì‹¤í—˜ ì‹œìŠ¤í…œ

**10% ë°ì´í„° ìƒ˜í”Œë§:**
```python
# Nikhilì²˜ëŸ¼
sample_episodes = train.sample(frac=0.1, random_state=42)

# ë¹ ë¥¸ ì‹¤í—˜
for config in CONFIGS:
    model = train_model(sample_episodes, config)
    score = evaluate(model)
    if score < threshold:
        # Full scale í•™ìŠµ
        full_model = train_model(train, config)
```

### TabDPT ê°™ì€ ìƒˆ ê¸°ë²• íƒìƒ‰

```
â–¡ DACON í† ë¡ ì—ì„œ ìƒìœ„ê¶Œ ê¸°ë²• í™•ì¸
â–¡ Tabular Transformer (TabDPT, TabNet, FT-Transformer)
â–¡ ì‘ì€ ì‹¤í—˜ìœ¼ë¡œ ë¨¼ì € í…ŒìŠ¤íŠ¸
```

---

## ğŸ“Š ë‘ ê¸€ ë¹„êµ

| í•­ëª© | Carla (ì²« ì°¸ê°€) | Nikhil (ìš°ìŠ¹) | ìš°ë¦¬ |
|------|-----------------|---------------|------|
| **ê²½í—˜** | ì´ˆë³´ | Kaggle Grandmaster | ì¤‘ê¸‰ |
| **ë„êµ¬** | ìˆ˜ë™ ì½”ë”© | Claude Code | ìˆ˜ë™ + ì¼ë¶€ AI |
| **í•µì‹¬ ì‹¤ìˆ˜** | ì§€í‘œ ì´í•´ ë¶€ì¡± | Data Leakage (3ë²ˆ!) | TBD (í™•ì¸ í•„ìš”) |
| **í•´ê²°** | í† ë¡  í¬ëŸ¼ | CLAUDE.md | ? |
| **ê²°ê³¼** | í•˜ìœ„ê¶Œ (í•™ìŠµ) | ìš°ìŠ¹ | 241ìœ„ (í•™ìŠµ ì¤‘) |
| **ê¸°ê°„** | 10ì£¼ | ? | 6ì£¼ (ì§„í–‰ ì¤‘) |

---

## ğŸš¨ ì¦‰ì‹œ ì‹¤í–‰ í•­ëª©

### 1. Data Leakage í™•ì¸ (ìµœìš°ì„ !)

```
â–¡ Zone 6x6 ëª¨ë¸: episode ë…ë¦½ì„± í™•ì¸
â–¡ ì „ì²´ íŒ¨ìŠ¤ í•™ìŠµ (356K): ë‹¤ë¥¸ episode ì •ë³´ ì‚¬ìš©í–ˆë‚˜?
â–¡ LSTM: episode ë…ë¦½ ì˜ˆì¸¡ ë³´ì¥ë˜ëŠ”ì§€?
```

**ì½”ë“œ ë¦¬ë·°:**
```python
# code/models/best/model_safe_fold13.py
# ì´ ëª¨ë¸ì´ episodeë³„ë¡œ ë…ë¦½ì ìœ¼ë¡œ ì˜ˆì¸¡í•˜ëŠ”ì§€ í™•ì¸!
```

### 2. CLAUDE.md ì‘ì„± (ì˜¤ëŠ˜)

```
â–¡ ìœ„ì˜ í…œí”Œë¦¿ ì‚¬ìš©
â–¡ ëŒ€íšŒ ê·œì¹™ ëª…ì‹œ
â–¡ Data Leakage ë°©ì§€ ê·œì¹™
â–¡ ì½”ë“œ ì„ í˜¸ë„
```

### 3. ì‹¤í—˜ ì‹œìŠ¤í…œ ê°œì„ 

```
â–¡ 10% ìƒ˜í”Œë¡œ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬
â–¡ ì‹¤í–‰ ì‹œê°„ ì¸¡ì • ì¶”ê°€
â–¡ ìŠ¹ìë§Œ full-scale
```

### 4. í† ë¡ /ì½”ë“œ í™œìš©

```
â–¡ DACON í† ë¡  ë§¤ì¼ 10ë¶„
â–¡ ìƒìœ„ê¶Œ ì½”ë“œ 1-2ê°œ ë¶„ì„
â–¡ ìƒˆë¡œìš´ ê¸°ë²• íƒìƒ‰ (TabNet ë“±)
```

---

## ğŸ“ í•µì‹¬ ë©”ì‹œì§€ (ì—…ë°ì´íŠ¸)

```
"Nikhilì˜ ê°€ì¥ í° êµí›ˆ: Data Leakage

3ë²ˆ ì œì¶œ, 3ë²ˆ ëª¨ë‘ ì‹¤íŒ¨. Claude Codeê°€ ë¯¸ë˜ ë°ì´í„°ë¥¼ ì‚¬ìš©í–ˆê¸° ë•Œë¬¸.
ê²€ì¦ ì ìˆ˜ëŠ” í›Œë¥­í–ˆì§€ë§Œ, ë¦¬ë”ë³´ë“œëŠ” í˜•í¸ì—†ì—ˆë‹¤.

í•œ ì¤„ì˜ í•„í„°ë§ ì½”ë“œê°€ ëª¨ë“  ê²ƒì„ ë°”ê¿¨ë‹¤:
trans_filt = transactions_df.filter((pl.col('dbd') >= 15))

ìš°ë¦¬ë„ ë§ˆì°¬ê°€ì§€ë‹¤.
Episode ë…ë¦½ì„±ì„ ë³´ì¥í•˜ëŠ”ê°€?
ë‹¤ë¥¸ episode ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šì•˜ëŠ”ê°€?

ì´ê²ƒë¶€í„° í™•ì¸í•˜ì. ì§€ê¸ˆ ë‹¹ì¥."
```

---

**ì‘ì„±ì¼:** 2025-12-15
**ì—…ë°ì´íŠ¸:** Medium ê¸€ 2ê°œ ë¶„ì„ ì™„ë£Œ
**ë‹¤ìŒ:** Data Leakage í™•ì¸, CLAUDE.md ì‘ì„±
