# AI Coding Constraints

> **ëª©ì :** AI(Claude Code)ê°€ ì½”ë“œ ì‘ì„± ì‹œ ë°˜ë“œì‹œ ì§€ì¼œì•¼ í•  ëª…ì‹œì  ì œì•½ ì¡°ê±´
> **ë™ê¸°:** Nikhil Mishraì˜ ì¡°ì–¸ - "Assume you're working with a smart junior engineer who needs explicit constraints"
> **ì‘ì„±ì¼:** 2025-12-15

---

## ğŸ¯ í•µì‹¬ ì›ì¹™

```
"Claude CodeëŠ” ë§¤ìš° ë˜‘ë˜‘í•œ ì£¼ë‹ˆì–´ ì—”ì§€ë‹ˆì–´ì…ë‹ˆë‹¤.
í•˜ì§€ë§Œ ëª…ì‹œì ì¸ ì œì•½ ì¡°ê±´ì´ ì—†ìœ¼ë©´ ì‹¤ìˆ˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ëª¨ë“  ê·œì¹™ì„ ëª…í™•í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”."
```

---

## ğŸš¨ ìµœìš°ì„  ê·œì¹™: Episode ë…ë¦½ì„±

### ëŒ€íšŒ ê·œì • (ì ˆëŒ€ ìœ„ë°˜ ê¸ˆì§€!)

> "ëª¨ë“  ì˜ˆì¸¡ì€ game_id-episode ë‹¨ìœ„ë¡œ ë…ë¦½ì ìœ¼ë¡œ ìˆ˜í–‰ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
> ì˜ˆì¸¡ì€ í•´ë‹¹ ì—í”¼ì†Œë“œ ë‚´ë¶€ì˜ ì‹œí€€ìŠ¤ ë°ì´í„°ë§Œì„ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ì•¼ í•˜ë©°,
> ë‹¤ë¥¸ ì—í”¼ì†Œë“œ(ë™ì¼ ê²½ê¸° ë‚´ ë‹¤ë¥¸ episode í¬í•¨)ì˜ ë°ì´í„°ë¥¼ í™œìš©í•œ ì¶”ë¡ ì€ ê¸ˆì§€ë©ë‹ˆë‹¤."

### âœ… ALWAYS (ë°˜ë“œì‹œ í•´ì•¼ í•  ê²ƒ)

```python
# 1. Episodeë³„ë¡œ í”¼ì²˜ ìƒì„±
df['feature'] = df.groupby('game_episode')['col'].transform(...)

# 2. Episodeë³„ë¡œ ë…ë¦½ ì²˜ë¦¬
for episode_id, group in df.groupby('game_episode'):
    process_episode(group)

# 3. Episode ë‚´ë¶€ ì‹œí€€ìŠ¤ë§Œ ì‚¬ìš©
episode_data = df[df['game_episode'] == target_episode]
features = create_features(episode_data)  # ì´ episodeë§Œ

# 4. Trainì—ì„œ ë°°ìš´ íŒ¨í„´ ì‚¬ìš© (OK!)
stats = train_df.groupby('zone').agg({'delta_x': 'median'})
pred = start_x + stats.loc[zone, 'delta_x']  # í•™ìŠµëœ í†µê³„
```

### âŒ NEVER (ì ˆëŒ€ í•˜ì§€ ë§ì•„ì•¼ í•  ê²ƒ)

```python
# 1. ë‹¤ë¥¸ episode ì •ë³´ ì‚¬ìš©
df['avg_across_episodes'] = df.groupby('game_id')['feature'].transform('mean')  # âŒ

# 2. Test episode ê°„ ì •ë³´ ê³µìœ 
test_df['global_avg'] = test_df['feature'].mean()  # âŒ ëª¨ë“  test episode í‰ê· 

# 3. ë™ì¼ game_idì˜ ë‹¤ë¥¸ episode ì •ë³´
same_game = df[df['game_id'] == current_game_id]
avg_feature = same_game['feature'].mean()  # âŒ ë‹¤ë¥¸ episode í¬í•¨

# 4. Episode ê²½ê³„ ë„˜ëŠ” Rolling/Shift
df['rolling'] = df['feature'].rolling(window=5).mean()  # âŒ episode ê²½ê³„ ë¬´ì‹œ
```

---

## ğŸ“‹ í”¼ì²˜ ìƒì„± ê·œì¹™

### Template: Episode-Safe Feature Engineering

```python
def create_features_episode_safe(df):
    """
    Episode ë…ë¦½ì„±ì„ ìœ ì§€í•˜ëŠ” í”¼ì²˜ ìƒì„±
    """
    df = df.copy()

    # âœ… Episodeë³„ Shift (ì´ì „ ê°’)
    df['prev_value'] = df.groupby('game_episode')['value'].shift(1).fillna(0)

    # âœ… Episodeë³„ Cumulative
    df['cumulative'] = df.groupby('game_episode')['value'].cumsum()

    # âœ… Episodeë³„ Count
    df['pass_count'] = df.groupby('game_episode').cumcount() + 1

    # âœ… Episodeë³„ í†µê³„
    df['episode_mean'] = df.groupby('game_episode')['value'].transform('mean')

    # âœ… ë…ë¦½ì  ê³„ì‚° (episode ë¬´ê´€)
    df['goal_distance'] = np.sqrt((105 - df['x'])**2 + (34 - df['y'])**2)

    return df
```

### ê¸ˆì§€ íŒ¨í„´

```python
# âŒ Episode ê²½ê³„ ë¬´ì‹œ
df['rolling_avg'] = df['value'].rolling(window=5).mean()

# âŒ Game-level aggregation (ë‹¤ë¥¸ episode í¬í•¨)
df['game_avg'] = df.groupby('game_id')['value'].transform('mean')

# âŒ Global statistics (train+test í˜¼í•©)
df['normalized'] = (df['value'] - df['value'].mean()) / df['value'].std()

# âœ… Train-only statistics (OK!)
train_mean = train_df['value'].mean()
df['normalized'] = (df['value'] - train_mean) / train_std
```

---

## ğŸ”„ Train/Test ë°ì´í„° ì²˜ë¦¬

### Template: Episode-Safe Train/Test Split

```python
# âœ… Episodeë³„ ë…ë¦½ ì²˜ë¦¬ (Train/Test ë™ì¼)
def preprocess_episodes(episodes, stats_from_train=None):
    """
    Args:
        episodes: list of (episode_id, group)
        stats_from_train: í•™ìŠµëœ í†µê³„ (Testì—ë§Œ ì œê³µ)
    """
    processed = []

    for episode_id, group in episodes:
        # Episode ë‚´ë¶€ í”¼ì²˜
        features = create_episode_features(group)

        # Trainì—ì„œ ë°°ìš´ íŒ¨í„´ ì ìš© (Testë§Œ)
        if stats_from_train is not None:
            features = apply_learned_stats(features, stats_from_train)

        processed.append((episode_id, features))

    return processed

# Train
train_episodes = list(train_df.groupby('game_episode'))
train_processed = preprocess_episodes(train_episodes)

# Train í†µê³„ í•™ìŠµ
train_stats = learn_statistics(train_processed)

# Test (Train í†µê³„ ì‚¬ìš©)
test_episodes = list(test_df.groupby('game_episode'))
test_processed = preprocess_episodes(test_episodes, stats_from_train=train_stats)
```

### Cross-Validation

```python
from sklearn.model_selection import GroupKFold

# âœ… GroupKFoldë¡œ game-level ë¶„ë¦¬
game_ids = np.array([ep_id.split('_')[0] for ep_id in episode_ids])
gkf = GroupKFold(n_splits=5)

for fold, (train_idx, val_idx) in enumerate(gkf.split(X, y, groups=game_ids)):
    X_train, X_val = X[train_idx], X[val_idx]
    y_train, y_val = y[train_idx], y[val_idx]

    # ê°™ì€ game_idì˜ episodeëŠ” ê°™ì€ fold
    # Episode ë…ë¦½ì„± ìœ ì§€
```

---

## ğŸ¯ ëª¨ë¸ ì˜ˆì¸¡ ê·œì¹™

### Template: Episode-Independent Prediction

```python
def predict_episodes(test_episodes, model, train_stats):
    """
    ê° episodeë¥¼ ë…ë¦½ì ìœ¼ë¡œ ì˜ˆì¸¡
    """
    predictions = []

    for episode_id, group in test_episodes:
        # âœ… ì´ episodeì˜ ë°ì´í„°ë§Œ ì‚¬ìš©
        episode_features = group[feature_cols].values

        # âœ… ì´ episodeì˜ start ì¢Œí‘œ
        start_x = group.iloc[-1]['start_x']
        start_y = group.iloc[-1]['start_y']

        # âœ… Trainì—ì„œ ë°°ìš´ íŒ¨í„´ ì‚¬ìš©
        zone = get_zone(start_x, start_y)
        delta = train_stats.loc[zone]

        # âœ… ì˜ˆì¸¡
        pred_x = np.clip(start_x + delta['x'], 0, 105)
        pred_y = np.clip(start_y + delta['y'], 0, 68)

        predictions.append({
            'game_episode': episode_id,
            'end_x': pred_x,
            'end_y': pred_y
        })

    return pd.DataFrame(predictions)
```

### ê¸ˆì§€ íŒ¨í„´

```python
# âŒ Batch ì˜ˆì¸¡ì—ì„œ ì •ë³´ ê³µìœ 
test_batch = test_df.iloc[batch_idx]
batch_mean = test_batch['feature'].mean()  # âŒ ì—¬ëŸ¬ episode ì •ë³´ í˜¼í•©
predictions = model.predict(test_batch)

# âœ… ì˜¬ë°”ë¥¸ Batch ì˜ˆì¸¡
for batch in test_batches:
    # Batch ë‚´ ê° episodeëŠ” ë…ë¦½ì 
    # Modelì€ ê° sampleì„ ë…ë¦½ì ìœ¼ë¡œ ì²˜ë¦¬
    predictions = model.predict(batch)
```

---

## ğŸ§ª ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸

### ìƒˆ ì½”ë“œ ì‘ì„± í›„ í•„ìˆ˜ í™•ì¸

- [ ] **í”¼ì²˜ ìƒì„±:** ëª¨ë“  í”¼ì²˜ê°€ `groupby('game_episode')` ì‚¬ìš©?
- [ ] **Train/Test:** ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬?
- [ ] **ì˜ˆì¸¡:** ê° episode ë…ë¦½ì ìœ¼ë¡œ ì˜ˆì¸¡?
- [ ] **ì •ë³´ ê³µìœ :** ë‹¤ë¥¸ episode ì •ë³´ ì‚¬ìš© ì•ˆ í•¨?
- [ ] **Cross-validation:** GroupKFold ì‚¬ìš©?

### ìê°€ ê²€ì¦ ì§ˆë¬¸

1. **"ì´ ì½”ë“œê°€ ë‹¤ë¥¸ episodeì˜ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ëŠ”ê°€?"**
   - ì‚¬ìš©í•˜ë©´ âŒ ìœ„ë°˜

2. **"Trainê³¼ Testë¥¼ ë‹¤ë¥´ê²Œ ì²˜ë¦¬í•˜ëŠ”ê°€?"**
   - ë‹¤ë¥´ë©´ âŒ ë¶„í¬ ë¶ˆì¼ì¹˜

3. **"Test episode ê°„ ì •ë³´ë¥¼ ê³µìœ í•˜ëŠ”ê°€?"**
   - ê³µìœ í•˜ë©´ âŒ ìœ„ë°˜

4. **"Episode ê²½ê³„ë¥¼ ë„˜ëŠ” ì—°ì‚°ì´ ìˆëŠ”ê°€?"**
   - ìˆìœ¼ë©´ âŒ ìœ„ë°˜

---

## ğŸš« ëŒ€íšŒ ê·œì¹™ ìœ„ë°˜ ê¸ˆì§€

### ì™¸ë¶€ ë°ì´í„° ê¸ˆì§€

```python
# âŒ ê¸ˆì§€
import requests
external_data = requests.get('https://api.example.com/data')

# âŒ ê¸ˆì§€
weather_df = pd.read_csv('external_weather_data.csv')

# âœ… í—ˆìš© (ì£¼ì–´ì§„ ë°ì´í„°ë§Œ)
train_df = pd.read_csv('train.csv')
test_df = pd.read_csv('test.csv')
```

### API í˜¸ì¶œ ê¸ˆì§€

```python
# âŒ ê¸ˆì§€
import openai
response = openai.ChatCompletion.create(...)

# âŒ ê¸ˆì§€
from anthropic import Anthropic
client = Anthropic()

# âœ… í—ˆìš© (ë¡œì»¬ ëª¨ë¸ë§Œ)
from transformers import AutoModel
model = AutoModel.from_pretrained('model_name')  # 2025.11.23 ì´ì „ ë²„ì „ë§Œ
```

### 2025.11.23 ì´ì „ ëª¨ë¸ë§Œ í—ˆìš©

```python
# âœ… í—ˆìš©
from transformers import AutoModel
model = AutoModel.from_pretrained('bert-base-uncased')  # 2023ë…„ ëª¨ë¸

# âŒ ê¸ˆì§€
model = AutoModel.from_pretrained('new-model-2025-12')  # 2025.11.23 ì´í›„
```

---

## ğŸ“ ì½”ë“œ ì‘ì„± ê°€ì´ë“œ

### 1. í”¼ì²˜ ìƒì„±

```python
# Template
def create_features(df):
    """
    CONSTRAINT: Episode ë…ë¦½ì„± ìœ ì§€
    """
    df = df.copy()

    # ëª¨ë“  groupbyëŠ” 'game_episode' ì‚¬ìš©
    df['feature1'] = df.groupby('game_episode')['col1'].transform(...)
    df['feature2'] = df.groupby('game_episode')['col2'].shift(1).fillna(0)

    # ë…ë¦½ì  ê³„ì‚° (episode ë¬´ê´€)
    df['feature3'] = some_calculation(df['col3'])

    return df
```

### 2. ëª¨ë¸ í•™ìŠµ

```python
# Template
def train_model(train_df):
    """
    CONSTRAINT: GroupKFold ì‚¬ìš©
    """
    # Episodeë³„ ì²˜ë¦¬
    train_episodes = list(train_df.groupby('game_episode'))

    # GroupKFold
    game_ids = np.array([ep_id.split('_')[0] for ep_id in episode_ids])
    gkf = GroupKFold(n_splits=5)

    for fold, (train_idx, val_idx) in enumerate(gkf.split(X, y, groups=game_ids)):
        # í•™ìŠµ
        model.fit(X[train_idx], y[train_idx])

    return model
```

### 3. ì˜ˆì¸¡

```python
# Template
def predict(test_df, model, train_stats):
    """
    CONSTRAINT: Episode ë…ë¦½ ì˜ˆì¸¡
    """
    predictions = []

    for episode_id, group in test_df.groupby('game_episode'):
        # ì´ episodeë§Œ ì‚¬ìš©
        pred = model.predict(group)

        predictions.append({
            'game_episode': episode_id,
            'end_x': pred[0],
            'end_y': pred[1]
        })

    return pd.DataFrame(predictions)
```

---

## âš ï¸ ì¼ë°˜ì ì¸ ì‹¤ìˆ˜ íŒ¨í„´

### 1. Global Statistics

```python
# âŒ ì˜ëª»ëœ ì˜ˆ
scaler = StandardScaler()
df['normalized'] = scaler.fit_transform(df[['feature']])  # Train+Test í˜¼í•©

# âœ… ì˜¬ë°”ë¥¸ ì˜ˆ
scaler = StandardScaler()
scaler.fit(train_df[['feature']])  # Trainë§Œ

train_df['normalized'] = scaler.transform(train_df[['feature']])
test_df['normalized'] = scaler.transform(test_df[['feature']])
```

### 2. Episode ê²½ê³„ ë¬´ì‹œ

```python
# âŒ ì˜ëª»ëœ ì˜ˆ
df['rolling_avg'] = df['feature'].rolling(window=5).mean()  # Episode ê²½ê³„ ë¬´ì‹œ

# âœ… ì˜¬ë°”ë¥¸ ì˜ˆ
df['rolling_avg'] = df.groupby('game_episode')['feature'].rolling(window=5).mean().reset_index(0, drop=True)
```

### 3. Test Leakage

```python
# âŒ ì˜ëª»ëœ ì˜ˆ
all_data = pd.concat([train_df, test_df])
all_data['feature'] = all_data['col'].transform(...)  # Train+Test í˜¼í•©

# âœ… ì˜¬ë°”ë¥¸ ì˜ˆ
train_df['feature'] = create_features(train_df)
test_df['feature'] = create_features(test_df)  # ë™ì¼ í•¨ìˆ˜, ë…ë¦½ ì²˜ë¦¬
```

---

## ğŸ“ í•™ìŠµí•œ êµí›ˆ (Nikhil ì‚¬ë¡€)

### ë¬¸ì œ ìƒí™©

```python
# Nikhilì˜ ì‹¤ìˆ˜ (3ë²ˆ ì‹¤íŒ¨)
# Constraint: 15ì¼ ì´í›„ ë°ì´í„°ë§Œ ì‚¬ìš©
transactions_df.filter(...)  # ì˜ëª»ëœ í•„í„°ë§ â†’ ë¯¸ë˜ ë°ì´í„° í¬í•¨

# í•´ê²° (1ë“±)
trans_filt = transactions_df.filter((pl.col('dbd') >= 15))  # ëª…ì‹œì  í•„í„°ë§
```

### êµí›ˆ

1. **ëª…ì‹œì  ì œì•½ ì¡°ê±´:**
   - "15ì¼ ì´í›„"ë¥¼ ì½”ë“œì— ëª…í™•íˆ í‘œí˜„: `>= 15`
   - "Episode ë…ë¦½"ì„ ì½”ë“œì— ëª…í™•íˆ í‘œí˜„: `groupby('game_episode')`

2. **ê°€ì •í•˜ì§€ ë§ ê²ƒ:**
   - AIëŠ” ì•”ë¬µì  ê·œì¹™ì„ ëª¨ë¥¼ ìˆ˜ ìˆìŒ
   - ëª¨ë“  ì œì•½ì„ ëª…ì‹œì ìœ¼ë¡œ ì‘ì„±

3. **ê²€ì¦ ì² ì €íˆ:**
   - ê° ë‹¨ê³„ë§ˆë‹¤ ê·œì¹™ ì¤€ìˆ˜ í™•ì¸
   - ì œì¶œ ì „ ìµœì¢… ê²€ì¦

---

## âœ… ì„±ê³µ ì‚¬ë¡€ (ìš°ë¦¬ í”„ë¡œì íŠ¸)

### Zone 6x6 ëª¨ë¸

```python
# Episodeë³„ ë…ë¦½ ì²˜ë¦¬
train_last = train_df.groupby('game_episode').last()

# GroupKFold
gkf = GroupKFold(n_splits=5)
for fold, (train_idx, val_idx) in enumerate(gkf.split(train_last, groups=game_ids)):
    ...

# Episode ë…ë¦½ ì˜ˆì¸¡
def predict_row(row):
    pred_x = row['start_x'] + stats.loc[row['key'], 'delta_x']
    return pred_x
```

**ê²°ê³¼:** âœ… Data Leakage ì—†ìŒ, Public 16.36 (241ìœ„)

### LSTM v3/v5 ëª¨ë¸

```python
# Episodeë³„ í”¼ì²˜
df['prev_dx'] = df.groupby('game_episode')['dx'].shift(1)
df['cumulative'] = df.groupby('game_episode')['dx'].cumsum()

# Episodeë³„ Sequence
for episode_id, group in episodes:
    input_seq = group[feature_cols].values[:-1]
    target = group.iloc[-1][['delta_x', 'delta_y']]
```

**ê²°ê³¼:** âœ… Data Leakage ì—†ìŒ, Public 17.29 (255ìœ„)

---

## ğŸ“š ì°¸ê³  ìë£Œ

- **ê²€ì¦ ë³´ê³ ì„œ:** `docs/DATA_LEAKAGE_VERIFICATION.md`
- **ëŒ€íšŒ ê·œì •:** `docs/COMPETITION_INFO.md`
- **Nikhil ì‚¬ë¡€:** `docs/COMPETITION_STRATEGIES_FROM_WINNERS.md`

---

## ğŸ”„ ì—…ë°ì´íŠ¸ ì´ë ¥

- **2025-12-15:** ìµœì´ˆ ì‘ì„± (Data Leakage ê²€ì¦ í›„)
- **ë‹¤ìŒ ì—…ë°ì´íŠ¸:** ìƒˆ ëª¨ë¸ ê°œë°œ ì‹œ

---

**ì‘ì„±ì:** Claude Sonnet 4.5
**ëª©ì :** AIê°€ ëŒ€íšŒ ê·œì¹™ì„ ëª…í™•íˆ ì´í•´í•˜ê³  ì¤€ìˆ˜í•˜ë„ë¡ ì§€ì›

---

*"The one line that saved everything: `trans_filt = transactions_df.filter((pl.col('dbd') >= 15))`"*
*- Nikhil Mishra, RedBus ëŒ€íšŒ ìš°ìŠ¹ì*
